{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section: Customize the simulation track and objects\n",
    "\n",
    "### Exercise 1 - Move the sim app to the working folder"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Clean the build directory if present\n",
    "!python3 sim_app_bundler.py --clean\n",
    "\n",
    "# # Untar the simapp bundle\n",
    "!python3 sim_app_bundler.py --untar ../deepracer-simapp.tar.gz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STOP - Exercise 1 is complete return to workshop to continue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6 - Preparing the AWS RoboMaker Bundle\n",
    "\n",
    "After making changes to the simulation application assets, re-bundle it using the Python file sim_app_bundler.py. We will upload the tar.gz file to the AWS RoboMaker arn later in the notebook.\n",
    "\n",
    "The compression may take longer depending on the instance type of your Amazon SageMaker notebook."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Most of the simapp files can be found here (Robomaker changes)\n",
    "# # bundle/opt/install/sagemaker_rl_agent/lib/python3.5/site-packages/\n",
    "# # bundle/opt/install/deepracer_simulation_environment/share/deepracer_simulation_environment/\n",
    "# # bundle/opt/install/deepracer_simulation_environment/lib/deepracer_simulation_environment/\n",
    "\n",
    "# # # Copying the notebook src/markov changes to the simapp (For sagemaker container)\n",
    " !rsync -av ./src/markov/ ./build/simapp/bundle/opt/install/sagemaker_rl_agent/lib/python3.5/site-packages/markov\n",
    "\n",
    " !python3 sim_app_bundler.py --tar /bundle/opt/install/sagemaker_rl_agent/lib/python3.5/site-packages/markov\n",
    "\n",
    " !python3 sim_app_bundler.py --tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STOP - Exercise 6 is complete return to workshop to continue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section: Update the neural network architecture\n",
    "\n",
    "### Exercise 12 - Copy custom files to S3 bucket so that Amazon SageMaker and AWS RoboMaker can pick them up\n",
    "\n",
    "**Very important**, remember to copy the edited files from ./src/ back into S3 where SageMaker and RoboMaker will pick them up"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # replace the environment file with your modified version\n",
    "# !cp src/markov/environments/deepracer_racetrack_env_cv2.py src/markov/environments/deepracer_racetrack_env.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_location = \"s3://%s/%s\" % (s3_bucket, s3_prefix)\n",
    "print(s3_location)\n",
    "\n",
    "# Clean up the previously uploaded files\n",
    "!aws s3 rm --recursive {s3_location}\n",
    "\n",
    "# Make any changes to the environment and preset files below and upload these files\n",
    "!aws s3 cp src/markov/environments/deepracer_racetrack_env.py {s3_location}/environments/deepracer_racetrack_env.py\n",
    "\n",
    "!aws s3 cp src/markov/rewards/default.py {s3_location}/rewards/reward_function.py\n",
    "\n",
    "!aws s3 cp src/markov/actions/model_metadata_10_state.json {s3_location}/model_metadata.json\n",
    "\n",
    "!aws s3 cp src/markov/presets/default.py {s3_location}/presets/preset.py\n",
    "#!aws s3 cp src/markov/presets/preset_attention_layer.py {s3_location}/presets/preset.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STOP - Exercise 12 is complete return to workshop to continue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section: Train the RL Model\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise XX - Notebook Imports"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'boto3'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-dad6644f8754>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mboto3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msagemaker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'boto3'"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import subprocess\n",
    "sys.path.append(\"common\")\n",
    "from misc import get_execution_role, wait_for_s3_object\n",
    "from docker_utils import build_and_push_docker_image\n",
    "from sagemaker.rl import RLEstimator, RLToolkit, RLFramework\n",
    "from time import gmtime, strftime\n",
    "import time\n",
    "from IPython.display import Markdown\n",
    "from markdown_helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise XX - Initialize basic parameters\n",
    "\n",
    "**Call-out, please check Amazon SageMaker instance type** - it has a cost implication\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the instance type\n",
    "# Be aware of the costs of these training instances. They will run as long as the training duration\n",
    "# We use ml.c4.2xlarge or ml.c5.2xlarge in the AWS DeepRacer console\n",
    "\n",
    "\n",
    "# instance_type = \"ml.c4.2xlarge\"\n",
    "instance_type = \"ml.p2.xlarge\"\n",
    "\n",
    "\n",
    "# Starting SageMaker session\n",
    "sage_session = sagemaker.session.Session()\n",
    "\n",
    "# Create unique job name.\n",
    "job_name_prefix = 'deepracer-notebook'\n",
    "\n",
    "# Duration of job in seconds (1 hours)\n",
    "job_duration_in_seconds = 3600 * 1\n",
    "\n",
    "# AWS Region\n",
    "aws_region = sage_session.boto_region_name\n",
    "if aws_region not in [\"us-west-2\", \"us-east-1\", \"eu-west-1\"]:\n",
    "    raise Exception(\"This notebook uses RoboMaker which is available only in US East (N. Virginia),\"\n",
    "                    \"US West (Oregon) and EU (Ireland). Please switch to one of these regions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise XX - Setup S3 bucket\n",
    "Set up the linkage and authentication to the S3 bucket that we want to use for checkpoint and metadata.\n",
    "\n",
    "#### **Note: This cell snapshots time to create folders** - training output will go to these folders. So running the code again means you are pointing to a different location. "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 bucket\n",
    "s3_bucket = sage_session.default_bucket()\n",
    "\n",
    "# SDK appends the job name and output folder\n",
    "s3_output_path = 's3://{}/'.format(s3_bucket)\n",
    "\n",
    "# Ensure that the S3 prefix contains the keyword 'sagemaker'\n",
    "s3_prefix = job_name_prefix + \"-sagemaker-\" + strftime(\"%y%m%d-%H%M%S\", gmtime())\n",
    "\n",
    "# Get the AWS account id of this account\n",
    "sts = boto3.client(\"sts\")\n",
    "account_id = sts.get_caller_identity()['Account']\n",
    "\n",
    "print(\"Using s3 bucket {}\".format(s3_bucket))\n",
    "print(\"Model checkpoints and other metadata will be stored at: \\ns3://{}/{}\".format(s3_bucket, s3_prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise XX - Build and push Docker image\n",
    "\n",
    "The file ./Dockerfile contains all the packages that are installed into the docker. Instead of using the default sagemaker container. We will be using this docker container. \n",
    "\n",
    "If the docker file is not yet present, this takes about 8 minutes to complete. It takes a few seconds on subsequent runs."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!docker rm -f $(docker ps -a -q);\n",
    "#!docker rmi -f $(docker images -q);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from copy_to_sagemaker_container import get_sagemaker_docker, copy_to_sagemaker_container, get_custom_image_name\n",
    "cpu_or_gpu = 'gpu' if instance_type.startswith('ml.p') else 'cpu'\n",
    "# repo name\n",
    "repository_short_name = job_name_prefix + \"-%s\" % cpu_or_gpu\n",
    "custom_image_name = get_custom_image_name(repository_short_name)\n",
    "\n",
    "try:\n",
    "    print(\"Copying files from your notebook to existing sagemaker container\")\n",
    "    sagemaker_docker_id = get_sagemaker_docker(repository_short_name)\n",
    "    copy_to_sagemaker_container(sagemaker_docker_id, repository_short_name)\n",
    "except Exception as e:\n",
    "    print(\"Creating sagemaker container\")\n",
    "    docker_build_args = {\n",
    "        'CPU_OR_GPU': cpu_or_gpu, \n",
    "        'AWS_REGION': boto3.Session().region_name,\n",
    "    }\n",
    "    custom_image_name = build_and_push_docker_image(repository_short_name, build_args=docker_build_args)\n",
    "    print(\"Using ECR image %s\" % custom_image_name)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise XX - Train the RL model using the Python SDK Script mode"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions = [\n",
    "    # Training> Name=main_level/agent, Worker=0, Episode=19, Total reward=-102.88, Steps=19019, Training iteration=1\n",
    "    {'Name': 'reward-training',\n",
    "     'Regex': '^Training>.*Total reward=(.*?),'},\n",
    "    \n",
    "    # Policy training> Surrogate loss=-0.32664725184440613, KL divergence=7.255815035023261e-06, Entropy=2.83156156539917, training epoch=0, learning_rate=0.00025\n",
    "    {'Name': 'ppo-surrogate-loss',\n",
    "     'Regex': '^Policy training>.*Surrogate loss=(.*?),'},\n",
    "     {'Name': 'ppo-entropy',\n",
    "     'Regex': '^Policy training>.*Entropy=(.*?),'},\n",
    "   \n",
    "    # Testing> Name=main_level/agent, Worker=0, Episode=19, Total reward=1359.12, Steps=20015, Training iteration=2\n",
    "    {'Name': 'reward-testing',\n",
    "     'Regex': '^Testing>.*Total reward=(.*?),'},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the RLEstimator for training RL jobs.\n",
    "\n",
    "1. Specify the source directory which has the environment file, preset and training code.\n",
    "2. Specify the entry point as the training code\n",
    "3. Specify the choice of RL toolkit and framework. This automatically resolves to the ECR path for the RL Container.\n",
    "4. Define the training parameters such as the instance count, instance type, job name, s3_bucket and s3_prefix for storing model checkpoints and metadata. **Only 1 training instance is supported for now.**\n",
    "4. Set the RLCOACH_PRESET as \"deepracer\" for this example.\n",
    "5. Define the metrics definitions that you are interested in capturing in your logs. These can also be visualized in CloudWatch and SageMaker Notebooks."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = RLEstimator(entry_point=\"training_worker.py\",\n",
    "                        source_dir='src',\n",
    "                        image_name=custom_image_name,\n",
    "                        dependencies=[\"common/\"],\n",
    "                        role=sagemaker_role,\n",
    "                        train_instance_type=instance_type,\n",
    "                        train_instance_count=1,\n",
    "                        output_path=s3_output_path,\n",
    "                        base_job_name=job_name_prefix,\n",
    "                        metric_definitions=metric_definitions,\n",
    "                        train_max_run=job_duration_in_seconds,\n",
    "                        hyperparameters={\n",
    "                            \"s3_bucket\": s3_bucket,\n",
    "                            \"s3_prefix\": s3_prefix,\n",
    "                            \"aws_region\": aws_region,\n",
    "                            \"preset_s3_key\": \"%s/presets/preset.py\"% s3_prefix,\n",
    "                            \"model_metadata_s3_key\": \"%s/model_metadata.json\" % s3_prefix,\n",
    "                            \"environment_s3_key\": \"%s/environments/deepracer_racetrack_env.py\" % s3_prefix,\n",
    "                        },\n",
    "                        subnets=deepracer_subnets,\n",
    "                        security_group_ids=deepracer_security_groups,\n",
    "                    )\n",
    "\n",
    "estimator.fit(wait=False)\n",
    "job_name = estimator.latest_training_job.job_name\n",
    "print(\"Training job: %s\" % job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise XX - Create the Kinesis video stream (optional)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kvs_stream_name = \"dr-kvs-{}\".format(job_name)\n",
    "\n",
    "!aws --region {aws_region} kinesisvideo create-stream --stream-name {kvs_stream_name} --media-type video/h264 --data-retention-in-hours 24\n",
    "print (\"Created kinesis video stream {}\".format(kvs_stream_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise XX - Start the Robomaker job"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robomaker = boto3.client(\"robomaker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise XX - Create Simulation Application"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robomaker_s3_key = 'robomaker/simulation_ws.tar.gz'\n",
    "robomaker_source = {'s3Bucket': s3_bucket,\n",
    "                    's3Key': robomaker_s3_key,\n",
    "                    'architecture': \"X86_64\"}\n",
    "simulation_software_suite={'name': 'Gazebo',\n",
    "                           'version': '7'}\n",
    "robot_software_suite={'name': 'ROS',\n",
    "                      'version': 'Kinetic'}\n",
    "rendering_engine={'name': 'OGRE',\n",
    "                  'version': '1.x'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise XX - Upload your customized simulation application to your s3 bucket"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your custom simApp\n",
    "!aws s3 cp ./build/output.tar.gz s3://{s3_bucket}/{robomaker_s3_key}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise XX - Create arn for the AWS RoboMaker simulation application"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create arn for AWS RoboMaker\n",
    "app_name = \"deepracer-notebook-application\" + strftime(\"%y%m%d-%H%M%S\", gmtime())\n",
    "\n",
    "print(app_name)\n",
    "try:\n",
    "    response = robomaker.create_simulation_application(name=app_name,\n",
    "                                                       sources=[robomaker_source],\n",
    "                                                       simulationSoftwareSuite=simulation_software_suite,\n",
    "                                                       robotSoftwareSuite=robot_software_suite,\n",
    "                                                       renderingEngine=rendering_engine)\n",
    "    simulation_app_arn = response[\"arn\"]\n",
    "    print(\"Created a new simulation app with ARN:\", simulation_app_arn)\n",
    "except Exception as e:\n",
    "    if \"AccessDeniedException\" in str(e):\n",
    "        display(Markdown(generate_help_for_robomaker_all_permissions(role)))\n",
    "        raise e\n",
    "    else:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section: Create multiple Rollouts\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Objective: Please increase number of workers to 2\n",
    "\n",
    "**Specify the number of roll-out workers** using the ***num_simulation_workers*** parameter."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_tracks = [\"reinvent_base\", # 0\n",
    "                    \"AWS_track\", # 1\n",
    "                    \"Tokyo_Training_track\", #2\n",
    "                    \"Virtual_May19_Train_track\", #3 (london)\n",
    "                    \"reInvent2018_36inch\", #4\n",
    "                    \"reInvent2018_mirror\", #5\n",
    "                    \"reInvent2019_track\"] #6\n",
    "\n",
    "training_tracks_indices = [6, 6, 5, 5]\n",
    "\n",
    "num_simulation_workers = len(training_tracks_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "envriron_vars = {\n",
    "    \"WORLD_NAME\": \"reinvent_base\",\n",
    "    \"KINESIS_VIDEO_STREAM_NAME\": \"SilverstoneStream\",\n",
    "    \"PYTHONPATHS\":\"wohooo this worked\",\n",
    "    \"ALTERNATE_DRIVING_DIRECTION\": \"false\",\n",
    "    \"SAGEMAKER_SHARED_S3_BUCKET\": s3_bucket,\n",
    "    \"SAGEMAKER_SHARED_S3_PREFIX\": s3_prefix,\n",
    "    \"TRAINING_JOB_ARN\": job_name,\n",
    "    \"APP_REGION\": aws_region,\n",
    "    \"METRIC_NAME\": \"TrainingRewardScore\",\n",
    "    \"METRIC_NAMESPACE\": \"AWSDeepRacer\",\n",
    "    \"REWARD_FILE_S3_KEY\": \"%s/rewards/reward_function.py\" % s3_prefix,\n",
    "    \"MODEL_METADATA_FILE_S3_KEY\": \"%s/model_metadata.json\" % s3_prefix,\n",
    "    \"METRICS_S3_BUCKET\": s3_bucket,\n",
    "    \"METRICS_S3_OBJECT_KEY\": s3_prefix + \"/training_metrics.json\",\n",
    "    \"TARGET_REWARD_SCORE\": \"None\",\n",
    "    \"ROBOMAKER_SIMULATION_JOB_ACCOUNT_ID\": account_id\n",
    "}\n",
    "\n",
    "simulation_application = {\"application\":simulation_app_arn,\n",
    "                          \"launchConfig\": {\"packageName\": \"deepracer_simulation_environment\",\n",
    "                                           \"launchFile\": \"distributed_training.launch\",\n",
    "                                           \"environmentVariables\": envriron_vars}\n",
    "                         }\n",
    "\n",
    "vpcConfig = {\"subnets\": deepracer_subnets,\n",
    "             \"securityGroups\": deepracer_security_groups,\n",
    "             \"assignPublicIp\": True}\n",
    "\n",
    "responses = []\n",
    "for job_no in range(num_simulation_workers):\n",
    "    simulation_application[\"launchConfig\"][\"environmentVariables\"][\"WORLD_NAME\"] = available_tracks[training_tracks_indices[job_no]]\n",
    "    client_request_token = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "    response =  robomaker.create_simulation_job(iamRole=sagemaker_role,\n",
    "                                            clientRequestToken=client_request_token,\n",
    "                                            maxJobDurationInSeconds=job_duration_in_seconds,\n",
    "                                            failureBehavior=\"Fail\",\n",
    "                                            simulationApplications=[simulation_application],\n",
    "                                            vpcConfig=vpcConfig\n",
    "                                            )\n",
    "    print(response)\n",
    "    responses.append(response)\n",
    "\n",
    "\n",
    "print(\"Created the following jobs:\")\n",
    "job_arns = [response[\"arn\"] for response in responses]\n",
    "for response in responses:\n",
    "    print(\"Job ARN\", response[\"arn\"]) \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the below output for log analysis and to determine the top performing models on the real world track."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"S3_MODEL_SAVE_PREFIX = '%s'\" %experiment_name)\n",
    "\n",
    "arn_str = list()\n",
    "for response in responses:\n",
    "    arn_str.append(response[\"arn\"].split('/')[-1])\n",
    "print('stream_name_list =' + str(arn_str))\n",
    "print(\"sagemaker_simapp_name_list = ['\" + str(job_name) + \"']\")\n",
    "\n",
    "print(\"s3_bucket = '%s'\" %s3_bucket)\n",
    "print(\"s3_prefix = '%s'\" %s3_prefix)\n",
    "print('simulation_app_arn = \"%s\"' %simulation_app_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the simulations in RoboMaker\n",
    "You can visit the RoboMaker console to visualize the simulations or run the following cell to generate the hyperlinks."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(generate_robomaker_links(job_arns, aws_region)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run evaluation in parallel"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"./src\")\n",
    "\n",
    "num_simulation_workers = 1\n",
    "\n",
    "\n",
    "envriron_vars = {\n",
    "    \"WORLD_NAME\": \"reInvent2019_track\",\n",
    "    \"KINESIS_VIDEO_STREAM_NAME\": \"SilverstoneStream\",\n",
    "    \"SAGEMAKER_SHARED_S3_BUCKET\": s3_bucket,\n",
    "    \"SAGEMAKER_SHARED_S3_PREFIX\": s3_prefix,\n",
    "    \"MODEL_S3_BUCKET\": s3_bucket,\n",
    "    \"PYTHONPATHS\":\"wohooo this worked\",\n",
    "    \"MODEL_S3_PREFIX\": s3_prefix,\n",
    "    \"ALTERNATE_DRIVING_DIRECTION\": \"false\",\n",
    "    \"APP_REGION\": aws_region,\n",
    "    \"MODEL_METADATA_FILE_S3_KEY\": \"%s/model_metadata.json\" % s3_prefix,\n",
    "    \"METRICS_S3_BUCKET\": s3_bucket,\n",
    "    \"METRICS_S3_OBJECT_KEY\": s3_prefix + \"/evaluation_metrics.json\",\n",
    "    \"NUMBER_OF_TRIALS\": \"5\", # Doesnt matter\n",
    "    \"ROBOMAKER_SIMULATION_JOB_ACCOUNT_ID\": account_id\n",
    "}\n",
    "\n",
    "simulation_application = {\n",
    "    \"application\":simulation_app_arn,\n",
    "    \"launchConfig\": {\n",
    "         \"packageName\": \"deepracer_simulation_environment\",\n",
    "         \"launchFile\": \"evaluation.launch\",\n",
    "         \"environmentVariables\": envriron_vars\n",
    "    }\n",
    "}\n",
    "                            \n",
    "vpcConfig = {\"subnets\": deepracer_subnets,\n",
    "             \"securityGroups\": deepracer_security_groups,\n",
    "             \"assignPublicIp\": True}\n",
    "\n",
    "responses_eval = []\n",
    "for job_no in range(num_simulation_workers):\n",
    "    response =  robomaker.create_simulation_job(clientRequestToken=strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime()),\n",
    "                                                outputLocation={ \n",
    "                                                  \"s3Bucket\": s3_bucket,\n",
    "                                                  \"s3Prefix\": s3_prefix\n",
    "                                                },\n",
    "                                                maxJobDurationInSeconds=job_duration_in_seconds*3,\n",
    "                                                iamRole=sagemaker_role,\n",
    "                                                failureBehavior=\"Continue\",\n",
    "                                                simulationApplications=[simulation_application],\n",
    "                                                vpcConfig=vpcConfig)\n",
    "    responses_eval.append(response)\n",
    "\n",
    "print('~~~ EVAL ~~~\\n')\n",
    "arn_str = list()\n",
    "for response in responses_eval:\n",
    "    arn_str.append(response[\"arn\"].split('/')[-1])\n",
    "print('eval_stream_name_list =' + str(arn_str))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: How to analyze your models and determine top 5 models to test on the real track"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to evaluation notebook?"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up RoboMaker and SageMaker training job\n",
    "\n",
    "Execute the cells below if you want to kill RoboMaker and SageMaker job."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cancelling robomaker job\n",
    "# for job_arn in job_arns:\n",
    "#     robomaker.cancel_simulation_job(job=job_arn)\n",
    "\n",
    "# # Stopping sagemaker training job\n",
    "# sage_session.sagemaker_client.stop_training_job(TrainingJobName=estimator._current_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Up Simulation Application Resource"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# robomaker.delete_simulation_application(application=simulation_app_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean your S3 bucket (Uncomment the awscli commands if you want to do it)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment if you only want to clean the s3 bucket\n",
    "# sagemaker_s3_folder = \"s3://{}/{}\".format(s3_bucket, s3_prefix)\n",
    "# !aws s3 rm --recursive {sagemaker_s3_folder}\n",
    "\n",
    "# robomaker_s3_folder = \"s3://{}/{}\".format(s3_bucket, job_name)\n",
    "# !aws s3 rm --recursive {robomaker_s3_folder}\n",
    "\n",
    "# robomaker_sim_app = \"s3://{}/{}\".format(s3_bucket, 'robomaker')\n",
    "# !aws s3 rm --recursive {robomaker_sim_app}\n",
    "\n",
    "# model_output = \"s3://{}/{}\".format(s3_bucket, s3_bucket)\n",
    "# !aws s3 rm --recursive {model_output}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the docker images\n",
    "Remove this only when you want to completely remove the docker or clean up the space of the sagemaker instance"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !docker rmi -f $(docker images -q)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python37464bita578b48ae7e2458da4e231f50ae310ad"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}